{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fef05f",
   "metadata": {},
   "source": [
    "# Final integrated notebook: Predicting Virality of Online News Articles\n",
    "\n",
    "This notebook is a single, sequential pipeline that implements Steps 1–8 from project planning: data cleaning, feature engineering, text features, EDA, baseline & ensemble models, clustering, neural models, scenario simulations, and a final predict API. It writes artifacts into `data/processed/`, `models/`, and `figures/` and is intended to be run top-to-bottom. Heavy tasks (Word2Vec, RNN) are optional and disabled by default.\n",
    "\n",
    "TL;DR: Run the cells in order. Use the `run_all(skip_heavy=True)` cell at the end to run a fast pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and environment setup\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optional imports (graceful)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "except Exception:\n",
    "    gensim = None\n",
    "\n",
    "try:\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    vader_available = True\n",
    "except Exception:\n",
    "    vader_available = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "except Exception:\n",
    "    tf = None\n",
    "\n",
    "print('Loaded core libraries. Optional packages: xgboost={}, shap={}, gensim={}, vader={}'.format(bool(xgb), bool(shap), bool(gensim), vader_available))\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Basic paths\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "FIGURES = PROJECT_ROOT / 'figures'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "for p in [DATA_PROCESSED, FIGURES, MODELS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities\n",
    "import json\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # strip, lower, replace spaces with underscores\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_dirs():\n",
    "    for p in [DATA_PROCESSED, FIGURES, MODELS_DIR]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_df(df, path: Path):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print('Saved:', path)\n",
    "\n",
    "\n",
    "def load_df_if_exists(path: Path):\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        return pd.read_csv(p)\n",
    "    return None\n",
    "\n",
    "print('Helper utilities defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7af64",
   "metadata": {},
   "source": [
    "## Step 1 — Data loading, typing, deduplication, and saving master CSV\n",
    "\n",
    "This section loads the raw CSV, normalizes columns, drops duplicates, inspects types and missingness, and writes `data/processed/cleaned_base.csv` and `data/processed/cleaned_with_features.csv` (after feature engineering cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: load and clean\n",
    "\n",
    "def load_and_clean(raw_path=DATA_RAW / 'OnlineNewsPopularity.csv'):\n",
    "    if not raw_path.exists():\n",
    "        raise FileNotFoundError(f\"Raw data not found: {raw_path}\")\n",
    "\n",
    "    df = pd.read_csv(raw_path)\n",
    "    print('Initial shape:', df.shape)\n",
    "\n",
    "    df = normalize_columns(df)\n",
    "    print('Columns normalized')\n",
    "\n",
    "    # quick dtype info and missing\n",
    "    print(df.dtypes.value_counts())\n",
    "    print('Total missing values:', df.isna().sum().sum())\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    print(f'Dropped {before-after} duplicate rows')\n",
    "\n",
    "    # canonicalize shares column if leading space exists\n",
    "    if 'shares' not in df.columns and ' shares' in df.columns:\n",
    "        df = df.rename(columns={' shares': 'shares'})\n",
    "\n",
    "    assert 'shares' in df.columns, 'shares column not found after normalization'\n",
    "\n",
    "    # Save cleaned base before feature engineering\n",
    "    save_df(df, DATA_PROCESSED / 'cleaned_base.csv')\n",
    "    return df\n",
    "\n",
    "# Run step 1 load\n",
    "try:\n",
    "    df_raw = load_and_clean()\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    df_raw = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0858a",
   "metadata": {},
   "source": [
    "## Step 2 — Core feature engineering (shares_log, viral, lengths, keywords, date parts)\n",
    "\n",
    "Define deterministic feature transformers and write `cleaned_with_features.csv`. Also include simple unit-style checks (assertions) that can be later converted to pytest tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a31e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering functions\n",
    "\n",
    "def add_core_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # ensure canonical shares\n",
    "    if 'shares' in df.columns:\n",
    "        df['shares'] = pd.to_numeric(df['shares'], errors='coerce').fillna(0).astype(int)\n",
    "    else:\n",
    "        raise KeyError('shares column missing')\n",
    "\n",
    "    df['shares_log'] = np.log1p(df['shares'])\n",
    "\n",
    "    # viral = top 10%\n",
    "    threshold = df['shares'].quantile(0.90)\n",
    "    df['viral'] = (df['shares'] >= threshold).astype(int)\n",
    "\n",
    "    # headline lengths\n",
    "    if 'title' in df.columns:\n",
    "        df['headline_char_len'] = df['title'].astype(str).map(len)\n",
    "        df['headline_word_len'] = df['title'].astype(str).str.split().map(len)\n",
    "    else:\n",
    "        # try common alternate column names\n",
    "        for c in ['headline', 'n_tokens_title']:\n",
    "            if c in df.columns:\n",
    "                df['headline_char_len'] = df[c].astype(str).map(len)\n",
    "                df['headline_word_len'] = df[c].astype(str).str.split().map(len)\n",
    "                break\n",
    "\n",
    "    # article length\n",
    "    if 'n_tokens_content' in df.columns:\n",
    "        df['article_length'] = df['n_tokens_content']\n",
    "\n",
    "    # keywords\n",
    "    if 'num_keywords' in df.columns:\n",
    "        df['keyword_count'] = df['num_keywords']\n",
    "    # keyword density\n",
    "    if 'keyword_count' in df.columns and 'article_length' in df.columns:\n",
    "        denom = df['article_length'].replace({0: np.nan})\n",
    "        df['keyword_density'] = df['keyword_count'] / denom\n",
    "\n",
    "    # weekday flags from possible one-hot columns\n",
    "    weekday_cols = [c for c in df.columns if c.startswith('weekday_is_')]\n",
    "    if weekday_cols:\n",
    "        df['weekday'] = df[weekday_cols].idxmax(axis=1).str.replace('weekday_is_', '')\n",
    "        df['is_weekend'] = df['weekday'].isin(['saturday', 'sunday']).astype(int)\n",
    "\n",
    "    # basic assertions\n",
    "    assert 'shares_log' in df.columns\n",
    "    assert 'viral' in df.columns\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run feature engineering if df_raw is present\n",
    "if df_raw is not None:\n",
    "    df_feat = add_core_features(df_raw)\n",
    "    save_df(df_feat, DATA_PROCESSED / 'cleaned_with_features.csv')\n",
    "else:\n",
    "    df_feat = None\n",
    "    print('Skipping feature engineering because raw data not loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a31d02d",
   "metadata": {},
   "source": [
    "## Step 2b — Text processing pipeline: TF-IDF, PCA, sentiment, optional Word2Vec\n",
    "\n",
    "This cell builds a TF-IDF -> PCA pipeline for headlines, computes sentiment scores (VADER/TextBlob fallback), and optionally trains a Word2Vec model to produce headline-average embeddings. Artifacts (vectorizer, pca, optional w2v) are saved under `models/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configurable flags\n",
    "TRAIN_WORD2VEC = False  # set True to train W2V (slow)\n",
    "W2V_SIZE = 100\n",
    "TFIDF_MAX_FEATURES = 10000\n",
    "PCA_COMPONENTS = 30\n",
    "\n",
    "\n",
    "def build_text_features(df, headline_col='title'):\n",
    "    df = df.copy()\n",
    "    if headline_col not in df.columns:\n",
    "        # try alternate\n",
    "        if 'headline' in df.columns:\n",
    "            headline_col = 'headline'\n",
    "        else:\n",
    "            print('No headline column found; skipping text features')\n",
    "            return df\n",
    "\n",
    "    texts = df[headline_col].astype(str).fillna('')\n",
    "\n",
    "    # TF-IDF\n",
    "    tf = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=(1,2), stop_words='english')\n",
    "    X_tfidf = tf.fit_transform(texts)\n",
    "    # save vectorizer\n",
    "    import joblib\n",
    "    joblib.dump(tf, MODELS_DIR / 'tfidf_vectorizer.joblib')\n",
    "    print('Saved TF-IDF vectorizer')\n",
    "\n",
    "    # PCA on TF-IDF to reduce dimensionality\n",
    "    n_comp = min(PCA_COMPONENTS, X_tfidf.shape[1])\n",
    "    pca = PCA(n_components=n_comp, random_state=SEED)\n",
    "    X_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "    joblib.dump(pca, MODELS_DIR / 'tfidf_pca.joblib')\n",
    "    print('Saved TF-IDF PCA')\n",
    "\n",
    "    # attach PCA columns\n",
    "    for i in range(X_pca.shape[1]):\n",
    "        df[f'headline_pc_{i+1}'] = X_pca[:, i]\n",
    "\n",
    "    # Sentiment\n",
    "    if vader_available:\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        sent = texts.map(lambda t: sia.polarity_scores(t))\n",
    "        sent_df = pd.DataFrame(list(sent))\n",
    "        sent_df.columns = [f'sent_{c}' for c in sent_df.columns]\n",
    "        df = pd.concat([df, sent_df.reset_index(drop=True)], axis=1)\n",
    "    else:\n",
    "        try:\n",
    "            from textblob import TextBlob\n",
    "            sent_df = texts.map(lambda t: pd.Series(TextBlob(t).sentiment._asdict()))\n",
    "            sent_df.columns = ['sent_polarity', 'sent_subjectivity']\n",
    "            df = pd.concat([df, sent_df.reset_index(drop=True)], axis=1)\n",
    "        except Exception:\n",
    "            print('No sentiment package available; skipping sentiment features')\n",
    "\n",
    "    # Word2Vec optional\n",
    "    if TRAIN_WORD2VEC and gensim is not None:\n",
    "        tokenized = [str(t).split() for t in texts]\n",
    "        w2v = Word2Vec(sentences=tokenized, vector_size=W2V_SIZE, window=5, min_count=1, workers=2, seed=SEED)\n",
    "        joblib.dump(w2v, MODELS_DIR / 'word2vec_headlines.joblib')\n",
    "        # average embeddings per headline\n",
    "        emb_matrix = []\n",
    "        for tokens in tokenized:\n",
    "            vecs = [w2v.wv[t] for t in tokens if t in w2v.wv]\n",
    "            if len(vecs) == 0:\n",
    "                emb = np.zeros(W2V_SIZE)\n",
    "            else:\n",
    "                emb = np.mean(vecs, axis=0)\n",
    "            emb_matrix.append(emb)\n",
    "        emb_matrix = np.vstack(emb_matrix)\n",
    "        # attach first 10 embedding dimensions as example\n",
    "        for i in range(min(10, emb_matrix.shape[1])):\n",
    "            df[f'head_emb_{i+1}'] = emb_matrix[:, i]\n",
    "\n",
    "    # Save enriched dataset\n",
    "    save_df(df, DATA_PROCESSED / 'features_complete.csv')\n",
    "    print('Text features built and saved to features_complete.csv')\n",
    "    return df\n",
    "\n",
    "# Build text features if base features exist\n",
    "if df_feat is not None:\n",
    "    df_text = build_text_features(df_feat)\n",
    "else:\n",
    "    df_text = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fddb12",
   "metadata": {},
   "source": [
    "## Step 3 — Exploratory Data Analysis (EDA)\n",
    "\n",
    "Produce quick plots and summaries used for interpretation and reporting. Saved outputs go to `figures/` and quick summary to `EDA_summary_output.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eda(df, out_dir=FIGURES):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(out_dir / 'EDA_summary_output.txt', 'w') as f:\n",
    "        f.write('EDA Summary\\n')\n",
    "\n",
    "    # Distribution of shares raw and log\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(df['shares'], bins=80)\n",
    "    plt.title('Distribution of shares (raw)')\n",
    "    plt.savefig(out_dir / 'shares_raw.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(df['shares_log'], bins=80)\n",
    "    plt.title('Distribution of shares (log)')\n",
    "    plt.savefig(out_dir / 'shares_log.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Top correlations with shares_log\n",
    "    corr = df.corr(numeric_only=True)['shares_log'].abs().sort_values(ascending=False)\n",
    "    top = corr.head(15).index\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(df[top].corr(), cmap='coolwarm')\n",
    "    plt.title('Top feature correlation heatmap')\n",
    "    plt.savefig(out_dir / 'top_corr_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "    print('EDA completed; figures saved to', out_dir)\n",
    "\n",
    "# run EDA if features are present\n",
    "if df_text is not None:\n",
    "    run_eda(df_text, FIGURES)\n",
    "else:\n",
    "    print('Skipping EDA: features not built')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1b709",
   "metadata": {},
   "source": [
    "## Step 4 — Baseline models with CV (Linear, Ridge, Lasso; optional Logistic for `viral`)\n",
    "\n",
    "This section contains utilities to train and evaluate baseline regressors/classifiers using 5-fold CV and saves results to `baseline_model_results.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7667ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, accuracy_score\n",
    "\n",
    "METRIC_RESULTS = []\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def evaluate_cv_regressors(X, y, cv=5):\n",
    "    models = {'Linear': LinearRegression(), 'Ridge': Ridge(), 'Lasso': Lasso()}\n",
    "    results = []\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=SEED)\n",
    "    for name, m in models.items():\n",
    "        scores = cross_validate(m, X, y, cv=kf, scoring=('neg_mean_squared_error', 'r2'))\n",
    "        rmse_mean = np.sqrt(-scores['test_neg_mean_squared_error'].mean())\n",
    "        r2_mean = scores['test_r2'].mean()\n",
    "        results.append({'model': name, 'rmse': rmse_mean, 'r2': r2_mean})\n",
    "    res_df = pd.DataFrame(results)\n",
    "    res_df.to_csv('baseline_model_results.csv', index=False)\n",
    "    print('Saved baseline_model_results.csv')\n",
    "    return res_df\n",
    "\n",
    "# Quick feature selection: numeric cols except target\n",
    "if df_text is not None:\n",
    "    numeric_cols = df_text.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c not in ['shares', 'shares_log', 'viral']]\n",
    "    X = df_text[numeric_cols].fillna(0)\n",
    "    y = df_text['shares_log']\n",
    "    baseline_results = evaluate_cv_regressors(X, y)\n",
    "else:\n",
    "    baseline_results = None\n",
    "    print('Skipping baseline models: features not built')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fd565",
   "metadata": {},
   "source": [
    "## Step 5 — Ensemble models (RandomForest, optional XGBoost) and explainability\n",
    "\n",
    "Train RandomForest with a small grid and XGBoost if available. Save best model(s) to `models/` and compute feature importances. If SHAP is available, compute summary plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e66c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "if df_text is not None:\n",
    "    # small grid\n",
    "    rf = RandomForestRegressor(random_state=SEED)\n",
    "    param_grid = {'n_estimators': [100, 200], 'max_depth': [6, 12]}\n",
    "    g = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=2)\n",
    "    g.fit(X, y)\n",
    "    print('Best RF params:', g.best_params_)\n",
    "    best_rf = g.best_estimator_\n",
    "    joblib.dump(best_rf, MODELS_DIR / 'best_random_forest.joblib')\n",
    "    print('Saved best_random_forest.joblib')\n",
    "\n",
    "    # XGBoost optional\n",
    "    if xgb is not None:\n",
    "        xgb_model = xgb.XGBRegressor(random_state=SEED, verbosity=0)\n",
    "        xgb_model.fit(X, y)\n",
    "        joblib.dump(xgb_model, MODELS_DIR / 'xgb_model.joblib')\n",
    "        print('Saved xgb_model.joblib')\n",
    "\n",
    "    # Feature importances\n",
    "    try:\n",
    "        importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        print(importances.head(20))\n",
    "    except Exception:\n",
    "        print('Could not compute feature importances')\n",
    "\n",
    "    # SHAP explainability (optional)\n",
    "    if shap is not None:\n",
    "        explainer = shap.Explainer(best_rf.predict, X)\n",
    "        shap_values = explainer(X)\n",
    "        shap.summary_plot(shap_values, X, show=False)\n",
    "        plt.savefig(FIGURES / 'shap_summary.png')\n",
    "        plt.close()\n",
    "        joblib.dump(explainer, MODELS_DIR / 'shap_explainer.joblib')\n",
    "        print('Saved SHAP explainer and plot')\n",
    "else:\n",
    "    print('Skipping ensemble training: features not built')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51058c1",
   "metadata": {},
   "source": [
    "## Step 6 — Clustering & cluster-feature integration (KMeans + optional advanced)\n",
    "\n",
    "Compute KMeans over a range of k, choose best by silhouette, save cluster labels as `cluster_kmeans` and write `features_with_clusters.csv` and cluster metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "if df_text is not None:\n",
    "    # use numeric + headline PCs\n",
    "    cluster_features = [c for c in df_text.columns if (c.startswith('headline_pc_') or df_text[c].dtype.kind in 'fi')]\n",
    "    cluster_features = [c for c in cluster_features if c not in ['shares','shares_log','viral']]\n",
    "    X_cluster = df_text[cluster_features].fillna(0)\n",
    "\n",
    "    sil_scores = {}\n",
    "    inertias = {}\n",
    "    for k in range(2, 11):\n",
    "        km = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "        labels = km.fit_predict(X_cluster)\n",
    "        sil = silhouette_score(X_cluster, labels)\n",
    "        sil_scores[k] = sil\n",
    "        inertias[k] = km.inertia_\n",
    "        print(f'k={k}: silhouette={sil:.4f}')\n",
    "\n",
    "    best_k = max(sil_scores, key=sil_scores.get)\n",
    "    print('Best k by silhouette:', best_k)\n",
    "    km_final = KMeans(n_clusters=best_k, random_state=SEED, n_init=10)\n",
    "    df_text['cluster_kmeans'] = km_final.fit_predict(X_cluster)\n",
    "    save_df(df_text, DATA_PROCESSED / 'features_with_clusters.csv')\n",
    "    pd.DataFrame({'k': list(sil_scores.keys()), 'silhouette': list(sil_scores.values()), 'inertia': list(inertias.values())}).to_csv(DATA_PROCESSED / 'cluster_metrics.csv', index=False)\n",
    "    print('Saved clustering outputs')\n",
    "else:\n",
    "    print('Skipping clustering: features not built')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a347f00",
   "metadata": {},
   "source": [
    "## Step 7 — Neural models (MLP regressor; optional LSTM headline classifier)\n",
    "\n",
    "Train a small MLP regressor using Keras for `shares_log`. The LSTM section is optional and will be skipped by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf is not None and df_text is not None:\n",
    "    from tensorflow.keras import layers, models, callbacks\n",
    "    # Simple MLP\n",
    "    features = [c for c in df_text.select_dtypes(include=[np.number]).columns if c not in ['shares','shares_log','viral']]\n",
    "    X_mlp = df_text[features].fillna(0).values\n",
    "    y_mlp = df_text['shares_log'].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_mlp_s = scaler.fit_transform(X_mlp)\n",
    "    joblib.dump(scaler, MODELS_DIR / 'mlp_scaler.joblib')\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X_mlp_s.shape[1],)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(X_mlp_s, y_mlp, validation_split=0.1, epochs=50, batch_size=64, callbacks=[es], verbose=1)\n",
    "    model.save(MODELS_DIR / 'mlp_regressor.keras')\n",
    "    print('Saved MLP regressor')\n",
    "else:\n",
    "    print('Skipping neural models: tensorflow missing or features not built')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3cfcd",
   "metadata": {},
   "source": [
    "## Step 8 — Final prediction API and scenario simulations\n",
    "\n",
    "Define `predict_from_dict()` which loads saved preprocessors and model artifacts and returns predicted shares and viral probability. Also include a few scenario simulations that tweak headline or metadata and show predicted deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51feed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts():\n",
    "    \"\"\"Load commonly used model and data artifacts. Returns a dict of artifacts.\"\"\"\n",
    "    artifacts = {}\n",
    "    import joblib\n",
    "    # load small artifacts if present\n",
    "    if (MODELS_DIR / 'tfidf_vectorizer.joblib').exists():\n",
    "        artifacts['tfidf'] = joblib.load(MODELS_DIR / 'tfidf_vectorizer.joblib')\n",
    "    if (MODELS_DIR / 'tfidf_pca.joblib').exists():\n",
    "        artifacts['pca'] = joblib.load(MODELS_DIR / 'tfidf_pca.joblib')\n",
    "    if (MODELS_DIR / 'best_random_forest.joblib').exists():\n",
    "        artifacts['rf'] = joblib.load(MODELS_DIR / 'best_random_forest.joblib')\n",
    "    if (MODELS_DIR / 'mlp_scaler.joblib').exists():\n",
    "        artifacts['mlp_scaler'] = joblib.load(MODELS_DIR / 'mlp_scaler.joblib')\n",
    "    # load dataset used for thresholds and population plots (if available)\n",
    "    if (DATA_PROCESSED / 'features_complete.csv').exists():\n",
    "        try:\n",
    "            artifacts['features_complete'] = pd.read_csv(DATA_PROCESSED / 'features_complete.csv')\n",
    "        except Exception:\n",
    "            artifacts['features_complete'] = None\n",
    "    return artifacts\n",
    "\n",
    "artifacts = load_artifacts()\n",
    "\n",
    "def _add_basic_features(df_rec):\n",
    "    df_rec = df_rec.copy()\n",
    "    if 'title' in df_rec.columns:\n",
    "        df_rec['headline_char_len'] = df_rec['title'].astype(str).map(len)\n",
    "        df_rec['headline_word_len'] = df_rec['title'].astype(str).str.split().map(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    if 'n_tokens_content' in df_rec.columns:\n",
    "        df_rec['article_length'] = df_rec['n_tokens_content']\n",
    "    if 'num_keywords' in df_rec.columns:\n",
    "        df_rec['keyword_count'] = df_rec['num_keywords']\n",
    "    if 'keyword_count' in df_rec.columns and 'article_length' in df_rec.columns:\n",
    "        denom = df_rec['article_length'].replace({0: np.nan})\n",
    "        df_rec['keyword_density'] = df_rec['keyword_count'] / denom\n",
    "    return df_rec\n",
    "\n",
    "def predict_from_dict(record: dict, artifacts=artifacts, return_raw=False):\n",
    "    \"\"\"Predict shares and viral probability for a single record (dict).\n",
    "    If RF is available it is used first. If not, falls back to the MLP model if present.\"\"\"\n",
    "    df_rec = pd.DataFrame([record])\n",
    "    df_rec = normalize_columns(df_rec)\n",
    "    df_rec = _add_basic_features(df_rec)\n",
    "\n",
    "    # text features: tfidf + pca -> headline_pc_*\n",
    "    if 'tfidf' in artifacts and 'pca' in artifacts and 'title' in df_rec.columns:\n",
    "        try:\n",
    "            X_t = artifacts['tfidf'].transform(df_rec['title'].astype(str))\n",
    "            X_p = artifacts['pca'].transform(X_t.toarray())\n",
    "            for i in range(X_p.shape[1]):\n",
    "                df_rec[f'headline_pc_{i+1}'] = X_p[:, i]\n",
    "        except Exception as e:\n",
    "            print('Warning: could not compute tfidf/pca for record:', e)\n",
    "\n",
    "    # prefer RF model\n",
    "    if 'rf' in artifacts:\n",
    "        model = artifacts['rf']\n",
    "        if hasattr(model, 'feature_names_in_'):\n",
    "            features = list(model.feature_names_in_)\n",
    "        else:\n",
    "            features = [c for c in df_rec.select_dtypes(include=[np.number]).columns]\n",
    "        # ensure all features present\n",
    "        for f in features:\n",
    "            if f not in df_rec.columns:\n",
    "                df_rec[f] = 0\n",
    "        X_rec = df_rec[features].fillna(0)\n",
    "        pred_log = model.predict(X_rec)[0]\n",
    "        pred_shares = float(np.expm1(pred_log))\n",
    "        # compute viral prob using dataset quantile if available\n",
    "        viral_prob = 0.0\n",
    "        if 'features_complete' in artifacts and artifacts['features_complete'] is not None:\n",
    "            try:\n",
    "                th = artifacts['features_complete']['shares'].quantile(0.90)\n",
    "                viral_prob = float(pred_shares >= th)\n",
    "            except Exception:\n",
    "                viral_prob = 0.0\n",
    "        out = {'pred_shares': pred_shares, 'pred_viral_prob': viral_prob}\n",
    "        if return_raw:\n",
    "            out['pred_log'] = float(pred_log)\n",
    "        return out\n",
    "\n",
    "    # fallback to MLP regressor\n",
    "    elif 'mlp_scaler' in artifacts and (MODELS_DIR / 'mlp_regressor.keras').exists():\n",
    "        scaler = artifacts['mlp_scaler']\n",
    "        try:\n",
    "            model = tf.keras.models.load_model(MODELS_DIR / 'mlp_regressor.keras')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('Could not load MLP model: ' + str(e))\n",
    "        features = [c for c in df_rec.select_dtypes(include=[np.number]).columns]\n",
    "        for f in features:\n",
    "            if f not in df_rec.columns:\n",
    "                df_rec[f] = 0\n",
    "        X_rec = df_rec[features].fillna(0)\n",
    "        Xs = scaler.transform(X_rec)\n",
    "        pred_log = model.predict(Xs)[0,0]\n",
    "        pred_shares = float(np.expm1(pred_log))\n",
    "        return {'pred_shares': pred_shares, 'pred_viral_prob': 0.0, 'pred_log': float(pred_log)}\n",
    "    else:\n",
    "        raise RuntimeError('No model artifacts available; please train an ensemble or MLP first')\n",
    "\n",
    "# Dataset-level diagnostics and visualization helpers\n",
    "def plot_predictions_vs_actual(artifacts=artifacts, sample_n=1000):\n",
    "    \"\"\"If dataset and model exist, predict across the dataset and create plots: predicted vs actual, residuals, and predicted distribution.\"\"\"\n",
    "    if 'features_complete' not in artifacts or artifacts['features_complete'] is None:\n",
    "        print('features_complete not available; skipping dataset plots')\n",
    "        return\n",
    "    df = artifacts['features_complete'].copy()\n",
    "    # choose numeric features matching model\n",
    "    if 'rf' in artifacts:\n",
    "        model = artifacts['rf']\n",
    "        if hasattr(model, 'feature_names_in_'):\n",
    "            features = list(model.feature_names_in_)\n",
    "        else:\n",
    "            features = [c for c in df.select_dtypes(include=[np.number]).columns if c not in ['shares','shares_log','viral']]\n",
    "        df_f = df[features].fillna(0)\n",
    "        preds_log = model.predict(df_f)\n",
    "        preds = np.expm1(preds_log)\n",
    "        df['pred_shares'] = preds\n",
    "        # scatter predicted vs actual (log scale)\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.scatterplot(x=np.log1p(df['shares']), y=np.log1p(df['pred_shares']), alpha=0.6)\n",
    "        plt.xlabel('Actual shares (log1p)')\n",
    "        plt.ylabel('Predicted shares (log1p)')\n",
    "        plt.title('Predicted vs Actual (log1p)')\n",
    "        plt.savefig(FIGURES / 'pred_vs_actual_log.png')\n",
    "        plt.close()\n",
    "        # residuals histogram\n",
    "        df['residual'] = np.log1p(df['pred_shares']) - np.log1p(df['shares'])\n",
    "        plt.figure(figsize=(7,4))\n",
    "        sns.histplot(df['residual'], bins=60)\n",
    "        plt.title('Residuals (pred - actual) on log scale')\n",
    "        plt.savefig(FIGURES / 'residuals_log.png')\n",
    "        plt.close()\n",
    "        print('Saved dataset prediction plots to', FIGURES)\n",
    "    else:\n",
    "        print('RF model not available; skipping dataset-level prediction plots')\n",
    "\n",
    "def run_scenario_simulations(base_record: dict, artifacts=artifacts, out_fig=FIGURES / 'scenario_simulations.png'):\n",
    "    \"\"\"Run a few deterministic scenarios (longer headline, more keywords, shorter content, all-caps) and plot predicted shares.\"\"\"\n",
    "    variants = []\n",
    "    base = base_record.copy()\n",
    "    variants.append(('base', base))\n",
    "    # longer headline\n",
    "    long_h = base.copy()\n",
    "    if 'title' in long_h:\n",
    "        long_h['title'] = long_h['title'] + ' ' + 'world'*10\n",
    "    variants.append(('long_headline', long_h))\n",
    "    # more keywords\n",
    "    more_k = base.copy()\n",
    "    more_k['num_keywords'] = int(base.get('num_keywords', 0)) + 10\n",
    "    variants.append(('more_keywords', more_k))\n",
    "    # shorter content\n",
    "    short_c = base.copy()\n",
    "    short_c['n_tokens_content'] = max(10, int(base.get('n_tokens_content', 100))//10)\n",
    "    variants.append(('short_content', short_c))\n",
    "    # all caps\n",
    "    caps = base.copy()\n",
    "    if 'title' in caps:\n",
    "        caps['title'] = caps['title'].upper()\n",
    "    variants.append(('all_caps', caps))\n",
    "\n",
    "    results = []\n",
    "    for name, rec in variants:\n",
    "        try:\n",
    "            out = predict_from_dict(rec, artifacts=artifacts, return_raw=True)\n",
    "            results.append({'scenario': name, 'pred_shares': out.get('pred_shares', None), 'pred_log': out.get('pred_log', None)})\n",
    "        except Exception as e:\n",
    "            results.append({'scenario': name, 'pred_shares': None, 'error': str(e)})\n",
    "    res_df = pd.DataFrame(results).set_index('scenario')\n",
    "    # plot\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=res_df.index, y=res_df['pred_shares'].fillna(0))\n",
    "    plt.ylabel('Predicted shares')\n",
    "    plt.title('Scenario simulation: predicted shares by variant')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_fig)\n",
    "    plt.close()\n",
    "    print('Saved scenario simulation figure to', out_fig)\n",
    "    return res_df\n",
    "\n",
    "# Example simulation (improved) and dataset plots if possible\n",
    "example = {'title': 'Breaking: Local team wins 3-1 in thrilling match', 'n_tokens_title': 7, 'n_tokens_content': 200, 'num_keywords': 5}\n",
    "try:\n",
    "    print('Example prediction:', predict_from_dict(example))\n",
    "    run_scenario_simulations(example)\n",
    "    plot_predictions_vs_actual()\n",
    "except Exception as e:\n",
    "    print('Prediction or plotting failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eee68b",
   "metadata": {},
   "source": [
    "## Diagnostics: holes and missing artifacts (programmatic)\n",
    "\n",
    "This cell programmatically checks for files we expect and prints a compact 'holes' list with suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_holes():\n",
    "    expected = {\n",
    "        'raw_csv': DATA_RAW / 'OnlineNewsPopularity.csv',\n",
    "        'cleaned_with_features': DATA_PROCESSED / 'cleaned_with_features.csv',\n",
    "        'features_complete': DATA_PROCESSED / 'features_complete.csv',\n",
    "        'tfidf_vectorizer': MODELS_DIR / 'tfidf_vectorizer.joblib',\n",
    "        'tfidf_pca': MODELS_DIR / 'tfidf_pca.joblib',\n",
    "        'best_rf': MODELS_DIR / 'best_random_forest.joblib',\n",
    "        'mlp_model': MODELS_DIR / 'mlp_regressor.keras'\n",
    "    }\n",
    "    missing = {k: str(p) for k,p in expected.items() if not p.exists()}\n",
    "    if missing:\n",
    "        print('Missing artifacts:')\n",
    "        for k,v in missing.items():\n",
    "            print(f' - {k}: {v}')\n",
    "    else:\n",
    "        print('All expected artifacts present')\n",
    "    # quick suggestions\n",
    "    if (MODELS_DIR / 'best_random_forest.joblib').exists() and not (MODELS_DIR / 'shap_explainer.joblib').exists():\n",
    "        print('Suggestion: run SHAP explainability (SHAP not found)')\n",
    "\n",
    "check_holes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c9e98",
   "metadata": {},
   "source": [
    "## Final notes and next steps\n",
    "\n",
    "- This notebook provides a linear, runnable pipeline. Use `TRAIN_WORD2VEC=True` to enable Word2Vec (slow) and set `TRAIN_RNN=True` if you want to train an LSTM/GRU (requires GPU for speed).\n",
    "- Missing artifacts are listed in the Diagnostics cell; typical quick fixes: run Step 1 & Step 2, then Step 4–5.\n",
    "- Recommended next work items: normalize column names in source scripts, pin `requirements.txt`, add unit tests for the transformer functions.\n",
    "\n",
    "Thank you — run the notebook top-to-bottom in a fresh environment after installing packages in `requirements.txt`. Use `skip_heavy=True` in the `run_all` cell to avoid long-running steps during quick demos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
